# -*- coding: utf-8 -*-
"""camera_motion_estimation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wwFaGjr2YXT4GC2da2jXmfBzCPmNVsWO
"""

import os
import time
import math
import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import mode
from sklearn.cluster import KMeans
from src.load_utils import get_kitti_tracking_img_filepaths, get_kitti_tracking_labels, get_frame_labels, draw_arrow_from_angle, vector_to_angle
from src.camera_motion_estimator import CameraMotionEstimator

# This function allows to calculate optical flow trajectories (Don't remember where I actually found the source code)
# The code also allows to specify step value. The greater the value the more sparse the calculation and visualisation
def calc_angl_n_transl(img, flow, step=8):
    '''
    input:
        - img - numpy array - image
        - flow - numpy array - optical flow
        - step - int - measurement of sparsity
    output:
        - angles - numpy array - array of angles of optical flow lines to the x-axis
        - translation - numpy array - array of length values for optical flow lines
        - lines - list - list of actual optical flow lines (where each line represents a trajectory of 
        a particular point in the image)
    '''

    angles = []
    translation = []

    h, w = img.shape[:2]
    y, x = np.mgrid[step / 2:h:step, step / 2:w:step].reshape(2, -1).astype(int)
    fx, fy = flow[y, x].T
    lines = np.vstack([x, y, x + fx, y + fy]).T.reshape(-1, 2, 2)
    lines = np.int32(lines + 0.5)

    for (x1, y1), (x2, y2) in lines:
        angle = math.atan2(- int(y2) + int(y1), int(x2) - int(x1)) * 180.0 / np.pi
        length = math.hypot(int(x2) - int(x1), - int(y2) + int(y1))
        translation.append(length)
        angles.append(angle)

    return np.array(angles), np.array(translation), lines


# function for drawing optical flow trajectories
def draw_flow(img, lines):
    '''
    input:
        - img - numpy array - image to draw on
        - lines - list - list of lines to draw
        - BGR image with visualised optical flow
    '''

    width_delay_ratio = 6
    height_delay_ratio = 5

    h, w = img.shape[:2]

    vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
    cv2.polylines(vis, lines, 0, (0, 255, 0))

    for (x1, y1), (x2, y2) in lines:
        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)

    return vis


# function that analyses optical flow information
def estimate_motion(angles, translation):
    '''
    Input:
        - angles - numpy array - array of angles of optical flow lines to the x-axis
        - translation - numpy array - array of length values for optical flow lines
    Output:
        - ang_mode - float - mode of angles of trajectories. can be used to determine the direction of movement
        - transl_mode - float - mode of translation values 
        - ratio - float - shows how different values of translation are across a pair of frames. allows to 
        conclude about the type of movement
        - steady - bool - show if there is almost no movement on the video at the moment
    '''

    # Get indices of nonzero opical flow values. We'll use just them
    nonzero = np.where(translation > 0)

    # Whether non-zero value is close to zero or not. Should be set as a thershold
    steady = np.mean(translation) < 0.5

    translation = translation[nonzero]
    transl_mode = mode(translation)[0][0]

    angles = angles[nonzero]
    ang_mode = mode(angles)[0][0]

    # cutt off twenty percent of the sorted list from both sides to get rid off outliers
    ten_percent = len(translation) // 10
    translations = sorted(translation)
    translations = translations[ten_percent: len(translations) - ten_percent]

    # cluster optical flow values and find out how different these cluster are
    # big difference (i.e. big ratio value) corresponds to panning, otherwise - trucking
    inliers = [tuple([inlier]) for inlier in translations]
    k_means = KMeans(n_clusters=3, random_state=0).fit(inliers)
    centers = sorted(k_means.cluster_centers_)
    ratio = centers[0] / centers[-1]

    return ang_mode, transl_mode, ratio, steady


def calculate_optical_flow(src_frames_dir, recording_num):
    img_filepaths = get_kitti_tracking_img_filepaths(src_frames_dir, recording_num)
    camera_motion_estimator = CameraMotionEstimator(img_filepaths)

    for i in range(1, len(img_filepaths)):
        frame = cv2.imread(img_filepaths[i])
        masked_frame = camera_motion_estimator.update(frame, [])

        cv2.imshow('frame', masked_frame)
        k = cv2.waitKey(0) & 0xff
        if k == 27:
            break


def estimate_motion_from_frames_sparse(src_frames_dir, dst_path, recording_num):
    img_filepaths = get_kitti_tracking_img_filepaths(src_frames_dir, recording_num)

    img1 = cv2.imread(img_filepaths[0])
    width = int(img1.shape[1])  # float `width`
    height = int(img1.shape[0])
    fps = 30

    print(width, height, fps)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(dst_path, fourcc, fps, (width, height))

    max_corners = 200
    feature_params = dict(maxCorners=max_corners,
                          qualityLevel=0.01,
                          minDistance=7,
                          blockSize=7)
    # Parameters for lucas kanade optical flow
    lk_params = dict(winSize=(15, 15),
                     maxLevel=2,
                     criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

    # SURF
    fast = cv2.FastFeatureDetector_create()

    # Create some random colors
    color = np.random.randint(0, 255, (max_corners, 3))
    # Take first frame and find corners in it
    previous_frame = cv2.imread(img_filepaths[0])
    old_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)
    # kp = fast.detect(old_gray, None)
    # p1 = cv2.KeyPoint_convert(kp)
    #
    # print(p0)
    # print('-------')
    # print(p1)

    # Create a mask image for drawing purposes
    mask = np.zeros_like(previous_frame)

    for i in range(1, len(img_filepaths)):
        frame = cv2.imread(img_filepaths[i])
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Recalculate keypoints and reset mask with paths after some time
        if i % 5 == 0:
            p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)
            mask = np.zeros_like(previous_frame)

        # calculate optical flow
        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)
        # Select good points
        if p1 is not None:
            good_new = p1[st == 1]
            good_old = p0[st == 1]

        # draw the tracks
        for i, (new, old) in enumerate(zip(good_new, good_old)):
            a, b = new.ravel()
            c, d = old.ravel()
            mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)
            frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)

        img = cv2.add(frame, mask)
        cv2.imshow('frame', img)
        k = cv2.waitKey(0) & 0xff
        if k == 27:
            break

        # Now update the previous frame and previous points
        old_gray = frame_gray.copy()
        p0 = good_new.reshape(-1, 1, 2)

    out.release()

def estimate_motion_from_frames(src_frames_dir, dst_path, recording_num):
    # set parameters for text drawn on the frames
    font = cv2.FONT_HERSHEY_COMPLEX
    fontScale = 2
    fontColor = (68, 148, 213)
    lineType = 3

    # initialise text variables to draw on frames
    angle = 'None'
    translation = 'None'
    motion = 'None'
    motion_type = 'None'
    # set counter value
    count = 1

    img_filepaths = get_kitti_tracking_img_filepaths(src_frames_dir, recording_num)

    img1 = cv2.imread(img_filepaths[0])
    width = int(img1.shape[1])  # float `width`
    height = int(img1.shape[0])
    fps = 30

    print(width, height, fps)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(dst_path, fourcc, fps, (width, height))

    previous_frame = cv2.imread(img_filepaths[0])
    # main loop
    for i in range(1, len(img_filepaths)):
        # time.sleep(0.05)
        next_frame = cv2.imread(img_filepaths[i])

        # if the image is colored
        if len(previous_frame.shape) == 3:
            prvs_gray = cv2.cvtColor(previous_frame.copy(), cv2.COLOR_BGR2GRAY)
            next_gray = cv2.cvtColor(next_frame.copy(), cv2.COLOR_BGR2GRAY)
        else:
            prvs_gray = previous_frame.copy()
            next_gray = next_frame.copy()

        if count == 3:
            # calculate optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

            # calculate trajectories and analyse them
            angles, transl, lines = calc_angl_n_transl(prvs_gray, flow)
            ang_mode, transl_mode, ratio, steady = estimate_motion(angles, transl)

            # draw trajectories on the frame
            #         next_gray = draw_flow(next_gray.copy(), lines)
            next_gray = cv2.cvtColor(next_gray.copy(), cv2.COLOR_GRAY2BGR)

            #         angle = str(round(ang_mode, 2))
            #         translation = str(round(transl_mode, 2))
            motion = 'No motion' if steady else round(ratio[0], 2)
            if isinstance(motion, float):
                motion_type = 'Panning' if motion > 0.6 else 'Trucking'

            count = 0

        # put values on the frame
        #     cv2.putText(next_gray, angle, (50,100), font, fontScale, fontColor, lineType)
        #     cv2.putText(next_gray, translation, (50,150), font, fontScale, fontColor, lineType)
        cv2.putText(next_gray, str(motion), (50, 90), font, fontScale, fontColor, lineType)
        cv2.putText(next_gray, motion_type, (50, 150), font, fontScale, fontColor, lineType)

        # write the frame to the new video
        out.write(next_gray)

        # update the previous frame
        previous_frame = next_frame.copy()
        count += 1

        cv2.imshow('frame', next_gray)
        c = cv2.waitKey(0)
        if c & 0xFF == ord('q'):
            break

    out.release()


def estimate_motion_mp4():
    # specify directory and file name
    src_path = "data/test_videos/bikes.mp4"
    dst_path = "results/results_videos/3.mp4"

    # initialise stream from video
    cap = cv2.VideoCapture(src_path)
    ret, prvs = cap.read()

    # initialise video writer
    frameRate = int(cap.get(cv2.CAP_PROP_FPS))
    codec = cv2.VideoWriter_fourcc(*'XVID')

    outputStream = cv2.VideoWriter(dst_path, codec, frameRate, (int(cap.get(3)), int(cap.get(4))))

    # set parameters for text drawn on the frames
    font = cv2.FONT_HERSHEY_COMPLEX
    fontScale = 2
    fontColor = (68, 148, 213)
    lineType = 3

    # initialise text variables to draw on frames
    angle = 'None'
    translation = 'None'
    motion = 'None'
    motion_type = 'None'
    # set counter value
    count = 1

    # main loop
    while True:
        # read a new frame
        ret, nxt = cap.read()

        if not ret:
            break

        # if the image is colored
        if len(prvs.shape) == 3:
            prvs_gray = cv2.cvtColor(prvs.copy(), cv2.COLOR_BGR2GRAY)
            next_gray = cv2.cvtColor(nxt.copy(), cv2.COLOR_BGR2GRAY)
        else:
            prvs_gray = prvs.copy()
            next_gray = nxt.copy()

        if count == 3:
            # calculate optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

            # calculate trajectories and analyse them
            angles, transl, lines = calc_angl_n_transl(prvs_gray, flow)
            ang_mode, transl_mode, ratio, steady = estimate_motion(angles, transl)

            # draw trajectories on the frame
            #         next_gray = draw_flow(next_gray.copy(), lines)
            next_gray = cv2.cvtColor(next_gray.copy(), cv2.COLOR_GRAY2BGR)

            #         angle = str(round(ang_mode, 2))
            #         translation = str(round(transl_mode, 2))
            motion = 'No motion' if steady else round(ratio[0], 2)
            if isinstance(motion, float):
                motion_type = 'Panning' if motion > 0.6 else 'Trucking'

            count = 0

        # put values on the frame
        #     cv2.putText(next_gray, angle, (50,100), font, fontScale, fontColor, lineType)
        #     cv2.putText(next_gray, translation, (50,150), font, fontScale, fontColor, lineType)
        cv2.putText(next_gray, str(motion), (50, 90), font, fontScale, fontColor, lineType)
        cv2.putText(next_gray, motion_type, (50, 150), font, fontScale, fontColor, lineType)

        # write the frame to the new video
        outputStream.write(next_gray)

        # update the previous frame
        prvs = nxt.copy()
        count += 1

        cv2.imshow('frame', next_gray)
        c = cv2.waitKey(1)
        if c & 0xFF == ord('q'):
            break

    outputStream.release()
